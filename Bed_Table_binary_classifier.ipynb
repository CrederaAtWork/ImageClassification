{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used this tutorial: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "code can be found here: https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1324 images belonging to 2 classes.\n",
      "Found 123 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 43s 340ms/step - loss: 0.4698 - accuracy: 0.7981 - val_loss: 0.4273 - val_accuracy: 0.7558\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 41s 328ms/step - loss: 0.2824 - accuracy: 0.8956 - val_loss: 0.0882 - val_accuracy: 0.9610\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.2169 - accuracy: 0.9218 - val_loss: 0.1532 - val_accuracy: 0.9584\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 39s 316ms/step - loss: 0.2027 - accuracy: 0.9232 - val_loss: 0.0178 - val_accuracy: 0.9438\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 39s 316ms/step - loss: 0.1655 - accuracy: 0.9378 - val_loss: 0.2372 - val_accuracy: 0.9584\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 566s 5s/step - loss: 0.1590 - accuracy: 0.9464 - val_loss: 0.4186 - val_accuracy: 0.9649\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 42s 335ms/step - loss: 0.1412 - accuracy: 0.9548 - val_loss: 0.0032 - val_accuracy: 0.9779\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 40s 320ms/step - loss: 0.1271 - accuracy: 0.9574 - val_loss: 0.0206 - val_accuracy: 0.9752\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 0.1290 - accuracy: 0.9604 - val_loss: 0.1497 - val_accuracy: 0.9662\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 40s 319ms/step - loss: 0.1075 - accuracy: 0.9598 - val_loss: 0.0434 - val_accuracy: 0.9753\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.1308 - accuracy: 0.9573 - val_loss: 0.0058 - val_accuracy: 0.9688\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 41s 331ms/step - loss: 0.0873 - accuracy: 0.9699 - val_loss: 0.0869 - val_accuracy: 0.9843\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 41s 330ms/step - loss: 0.0901 - accuracy: 0.9719 - val_loss: 8.0971e-04 - val_accuracy: 0.9844\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.0925 - accuracy: 0.9719 - val_loss: 0.2255 - val_accuracy: 0.9909\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 42s 333ms/step - loss: 0.1040 - accuracy: 0.9679 - val_loss: 0.4425 - val_accuracy: 0.9286\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 41s 326ms/step - loss: 0.1012 - accuracy: 0.9634 - val_loss: 2.6662e-05 - val_accuracy: 0.9673\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 43s 343ms/step - loss: 0.0847 - accuracy: 0.9729 - val_loss: 0.1944 - val_accuracy: 0.9753\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 43s 348ms/step - loss: 0.0992 - accuracy: 0.9754 - val_loss: 0.0081 - val_accuracy: 0.9688\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 43s 348ms/step - loss: 0.0743 - accuracy: 0.9775 - val_loss: 0.0107 - val_accuracy: 0.9727\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.0923 - accuracy: 0.9769 - val_loss: 8.7200e-05 - val_accuracy: 0.9686\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.0812 - accuracy: 0.9759 - val_loss: 0.0023 - val_accuracy: 0.9753\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.0669 - accuracy: 0.9820 - val_loss: 0.0118 - val_accuracy: 0.9844\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 40s 321ms/step - loss: 0.0887 - accuracy: 0.9754 - val_loss: 0.0062 - val_accuracy: 0.9740\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.0806 - accuracy: 0.9760 - val_loss: 7.6051e-04 - val_accuracy: 0.9765\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 0.0844 - accuracy: 0.9749 - val_loss: 0.0042 - val_accuracy: 0.9584\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.0565 - accuracy: 0.9820 - val_loss: 2.4559e-05 - val_accuracy: 0.9584\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.0806 - accuracy: 0.9770 - val_loss: 0.2144 - val_accuracy: 0.9831\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.0774 - accuracy: 0.9814 - val_loss: 4.1328e-04 - val_accuracy: 0.9752\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 40s 319ms/step - loss: 0.0641 - accuracy: 0.9809 - val_loss: 0.1596 - val_accuracy: 0.9753\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 42s 339ms/step - loss: 0.0847 - accuracy: 0.9790 - val_loss: 0.1349 - val_accuracy: 0.9636\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 42s 338ms/step - loss: 0.0572 - accuracy: 0.9795 - val_loss: 0.0041 - val_accuracy: 0.9766\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 41s 328ms/step - loss: 0.0601 - accuracy: 0.9839 - val_loss: 3.3821e-05 - val_accuracy: 0.9516\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.0511 - accuracy: 0.9804 - val_loss: 5.8674e-06 - val_accuracy: 0.9844\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 43s 345ms/step - loss: 0.0614 - accuracy: 0.9825 - val_loss: 9.4345e-06 - val_accuracy: 0.9753\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.0866 - accuracy: 0.9719 - val_loss: 2.4382e-04 - val_accuracy: 0.9429\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.0816 - accuracy: 0.9815 - val_loss: 3.0505e-05 - val_accuracy: 0.9752\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 43s 344ms/step - loss: 0.0606 - accuracy: 0.9854 - val_loss: 0.6196 - val_accuracy: 0.9338\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 41s 327ms/step - loss: 0.1042 - accuracy: 0.9704 - val_loss: 9.4976e-04 - val_accuracy: 0.9831\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.0596 - accuracy: 0.9809 - val_loss: 0.6399 - val_accuracy: 0.9831\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 41s 329ms/step - loss: 0.0861 - accuracy: 0.9830 - val_loss: 0.0068 - val_accuracy: 0.9190\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 41s 324ms/step - loss: 0.0632 - accuracy: 0.9819 - val_loss: 0.3929 - val_accuracy: 0.9753\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 43s 348ms/step - loss: 0.0943 - accuracy: 0.9770 - val_loss: 0.0582 - val_accuracy: 0.9844\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 43s 344ms/step - loss: 0.0692 - accuracy: 0.9844 - val_loss: 1.0905e-05 - val_accuracy: 0.9831\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 41s 327ms/step - loss: 0.0715 - accuracy: 0.9839 - val_loss: 0.0088 - val_accuracy: 0.9686\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.0928 - accuracy: 0.9795 - val_loss: 0.0290 - val_accuracy: 0.9766\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 40s 324ms/step - loss: 0.0929 - accuracy: 0.9784 - val_loss: 0.0261 - val_accuracy: 0.9857\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 40s 320ms/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 1.1194e-06 - val_accuracy: 0.9662\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.0983 - accuracy: 0.9829 - val_loss: 1.4158e-05 - val_accuracy: 0.9739\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.0566 - accuracy: 0.9845 - val_loss: 1.8471e-06 - val_accuracy: 0.9844\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 41s 327ms/step - loss: 0.0895 - accuracy: 0.9809 - val_loss: 0.4252 - val_accuracy: 0.9662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xb2cc552b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bed_table_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We achieved 96% accuracy on the validation data!\n",
    "We can try to tune the hyperparameters to improve this still. Additionally, we can see if there are any data augmentation or preprocessing we can do to improve this score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
